{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"603_CV_Project_2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMr6hoCOjwcvFMORPSyo5T0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ekZ8Y4_cONhA","executionInfo":{"status":"ok","timestamp":1642644320700,"user_tz":300,"elapsed":6248,"user":{"displayName":"Yanliang Qi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06013840371136516913"}},"outputId":"63943069-8b2f-4cd8-81f1-c94656027492"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n"]}],"source":["import torch\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"]},{"cell_type":"markdown","source":["# Project 2 Action Recognition on UCF101\n","In this project, we will implement classical action recognition method C3D on UCF101 video dataset. The UCF101 video dataset is an action recognition dataset of realistic action videos, collected from YouTube, having 101 categories. The following picture illustrates some action categories in the dataset.\n","\n","![](https://drive.google.com/uc?export=view&id=1yfaNJp-m1rtPHbZlN66LBmf31p5Bnt74)\n","\n","Before you start this project, please read the ICCV 2015 paper (Learning Spatiotemporal Features with 3D Convolutional Networks). You need to answer Q1, Q2, Q3, Q4 and Q5 in a report."],"metadata":{"id":"ZIE4DnLTOsY-"}},{"cell_type":"markdown","source":["## Q1\n","\n","Prepare dataset. Visit the UCF101 official website and download the video dataset (https://www.crcv.ucf.edu/datasets/human-actions/ucf101/UCF101.rar). Unzip the dataset\n","\n","in the “data” folder. Then read the dataset.py file in the “dataloaders” folder and answer how this code split videos into training, validation and testing dataset.\n"],"metadata":{"id":"h6QW41_tPlPd"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","path_of_dataset=\n"],"metadata":{"id":"3oQ8DoSgQjPY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# You can also using wget to download dataset into colab. Be careful not to exceed the colab daily limitation.\n","#!wget --no-check-certificate https://www.crcv.ucf.edu/datasets/human-actions/ucf101/UCF101.rar -P your_target_path\n","#unrar x -Y \"your_target_path\" \"unzipp path\""],"metadata":{"id":"ywwXG9qeXY7h","executionInfo":{"status":"ok","timestamp":1642686950384,"user_tz":300,"elapsed":3,"user":{"displayName":"Yanliang Qi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06013840371136516913"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### dataloaders\n","\n","\n"],"metadata":{"id":"yTYo_g4HXQi3"}},{"cell_type":"code","source":["#you need to load datasetloader folder from google drive\n","import os\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import cv2\n","import numpy as np\n","from torch.utils.data import Dataset\n","\n","\n","\n","class VideoDataset(Dataset):\n","    \"\"\"A Dataset for a folder of videos. Expects the directory structure to be\n","    directory->[train/val/test]->[class labels]->[videos]. Initializes with a list\n","    of all file names, along with an array of labels, with label being automatically\n","    inferred from the respective folder names.\n","\n","        Args:\n","            dataset (str): Name of dataset. Defaults to 'ucf101'.\n","            split (str): Determines which folder of the directory the dataset will read from. Defaults to 'train'.\n","            clip_len (int): Determines how many frames are there in each clip. Defaults to 16.\n","            preprocess (bool): Determines whether to preprocess dataset. Default is False.\n","    \"\"\"\n","\n","    def __init__(self, dataset='ucf101', split='train', clip_len=16, preprocess=False):\n","        self.root_dir, self.output_dir = Path.db_dir(dataset)\n","        folder = os.path.join(self.output_dir, split)\n","        self.clip_len = clip_len\n","        self.split = split\n","\n","        # The following three parameters are chosen as described in the paper section 4.1\n","        self.resize_height = 128\n","        self.resize_width = 171\n","        self.crop_size = 112\n","\n","        if not self.check_integrity():\n","            raise RuntimeError('Dataset not found or corrupted.' +\n","                               ' You need to download it from official website.')\n","\n","        if (not self.check_preprocess()) or preprocess:\n","            print('Preprocessing of {} dataset, this will take long, but it will be done only once.'.format(dataset))\n","            self.preprocess()\n","        # Obtain all the filenames of files inside all the class folders\n","        # Going through each class folder one at a time\n","        self.fnames, labels = [], []\n","        for label in sorted(os.listdir(folder)):\n","            for fname in os.listdir(os.path.join(folder, label)):\n","                self.fnames.append(os.path.join(folder, label, fname))\n","                labels.append(label)\n","\n","        assert len(labels) == len(self.fnames)\n","        print('Number of {} videos: {:d}'.format(split, len(self.fnames)))\n","\n","        # Prepare a mapping between the label names (strings) and indices (ints)\n","        self.label2index = {label: index for index, label in enumerate(sorted(set(labels)))}\n","        # Convert the list of label names into an array of label indices\n","\n","        self.label_array = np.array([self.label2index[label] for label in labels], dtype=int)\n","\n","        if dataset == \"ucf101\":\n","            if not os.path.exists('dataloaders/ucf_labels.txt'):\n","                with open('dataloaders/ucf_labels.txt', 'w') as f:\n","                    for id, label in enumerate(sorted(self.label2index)):\n","                        f.writelines(str(id+1) + ' ' + label + '\\n')\n","\n","        elif dataset == 'hmdb51':\n","            if not os.path.exists('dataloaders/hmdb_labels.txt'):\n","                with open('dataloaders/hmdb_labels.txt', 'w') as f:\n","                    for id, label in enumerate(sorted(self.label2index)):\n","                        f.writelines(str(id+1) + ' ' + label + '\\n')\n","\n","\n","    def __len__(self):\n","        return len(self.fnames)\n","\n","    def __getitem__(self, index):\n","        # Loading and preprocessing.\n","        buffer = self.load_frames(self.fnames[index])\n","        buffer = self.crop(buffer, self.clip_len, self.crop_size)\n","        labels = np.array(self.label_array[index])\n","\n","        if self.split == 'test':\n","            # Perform data augmentation\n","            buffer = self.randomflip(buffer)\n","        buffer = self.normalize(buffer)\n","        buffer = self.to_tensor(buffer)\n","        return buffer,labels\n","        # return torch.from_numpy(buffer), torch.from_numpy(labels)\n","\n","    def check_integrity(self):\n","        if not os.path.exists(self.root_dir):\n","            return False\n","        else:\n","            return True\n","\n","    def check_preprocess(self):\n","        # TODO: Check image size in output_dir\n","        if not os.path.exists(self.output_dir):\n","            return False\n","        elif not os.path.exists(os.path.join(self.output_dir, 'train')):\n","            return False\n","\n","        for ii, video_class in enumerate(os.listdir(os.path.join(self.output_dir, 'train'))):\n","            for video in os.listdir(os.path.join(self.output_dir, 'train', video_class)):\n","                video_name = os.path.join(os.path.join(self.output_dir, 'train', video_class, video),\n","                                    sorted(os.listdir(os.path.join(self.output_dir, 'train', video_class, video)))[0])\n","                image = cv2.imread(video_name)\n","                if np.shape(image)[0] != 128 or np.shape(image)[1] != 171:\n","                    return False\n","                else:\n","                    break\n","\n","            if ii == 10:\n","                break\n","\n","        return True\n","\n","    def preprocess(self):\n","         if not os.path.exists(self.output_dir):\n","            os.mkdir(self.output_dir)\n","            os.mkdir(os.path.join(self.output_dir, 'train'))\n","            os.mkdir(os.path.join(self.output_dir, 'val'))\n","            os.mkdir(os.path.join(self.output_dir, 'test'))\n","\n","        # Split train/val/test sets\n","         for file in os.listdir(self.root_dir):\n","            file_path = os.path.join(self.root_dir, file)\n","            video_files = [name for name in os.listdir(file_path)]\n","\n","            train_and_valid, test = train_test_split(video_files, test_size=0.2, random_state=42)\n","            train, val = train_test_split(train_and_valid, test_size=0.2, random_state=42)\n","\n","            train_dir = os.path.join(self.output_dir, 'train', file)\n","            val_dir = os.path.join(self.output_dir, 'val', file)\n","            test_dir = os.path.join(self.output_dir, 'test', file)\n","\n","            if not os.path.exists(train_dir):\n","\n","                train_dir = train_dir.replace(\"\\ \", \"\\\\\")\n","                print(\"train \" + train_dir)\n","                os.mkdir(train_dir)\n","            if not os.path.exists(val_dir):\n","                val_dir = val_dir.replace(\"\\ \", \"\\\\\")\n","                os.mkdir(val_dir)\n","            if not os.path.exists(test_dir):\n","                test_dir = test_dir.replace(\"\\ \", \"\\\\ \")\n","                os.mkdir(test_dir)\n","\n","            for video in train:\n","                self.process_video(video, file, train_dir)\n","\n","            for video in val:\n","                self.process_video(video, file, val_dir)\n","\n","            for video in test:\n","                self.process_video(video, file, test_dir)\n","\n","         print('Preprocessing finished.')\n","\n","    def process_video(self, video, action_name, save_dir):\n","        # Initialize a VideoCapture object to read video data into a numpy array\n","        video_filename = video.split('.')[0]\n","        if not os.path.exists(os.path.join(save_dir, video_filename)):\n","            os.mkdir(os.path.join(save_dir, video_filename))\n","\n","        capture = cv2.VideoCapture(os.path.join(self.root_dir, action_name, video))\n","\n","        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n","        frame_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        frame_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","        # Make sure splited video has at least 16 frames\n","        EXTRACT_FREQUENCY = 4\n","        if frame_count // EXTRACT_FREQUENCY <= 16:\n","            EXTRACT_FREQUENCY -= 1\n","            if frame_count // EXTRACT_FREQUENCY <= 16:\n","                EXTRACT_FREQUENCY -= 1\n","                if frame_count // EXTRACT_FREQUENCY <= 16:\n","                    EXTRACT_FREQUENCY -= 1\n","\n","        count = 0\n","        i = 0\n","        retaining = True\n","\n","        while (count < frame_count and retaining):\n","            retaining, frame = capture.read()\n","            if frame is None:\n","                continue\n","\n","            if count % EXTRACT_FREQUENCY == 0:\n","                if (frame_height != self.resize_height) or (frame_width != self.resize_width):\n","                    frame = cv2.resize(frame, (self.resize_width, self.resize_height))\n","                cv2.imwrite(filename=os.path.join(save_dir, video_filename, '0000{}.jpg'.format(str(i))), img=frame)\n","                i += 1\n","            count += 1\n","\n","        # Release the VideoCapture once it is no longer needed\n","        capture.release()\n","\n","    def randomflip(self, buffer):\n","        \"\"\"Horizontally flip the given image and ground truth randomly with a probability of 0.5.\"\"\"\n","\n","        if np.random.random() < 0.5:\n","            for i, frame in enumerate(buffer):\n","                frame = cv2.flip(buffer[i], flipCode=1)\n","                buffer[i] = cv2.flip(frame, flipCode=1)\n","\n","        return buffer\n","\n","\n","    def normalize(self, buffer):\n","        for i, frame in enumerate(buffer):\n","            frame -= np.array([[[90.0, 98.0, 102.0]]])\n","            buffer[i] = frame\n","\n","        return buffer\n","\n","    def to_tensor(self, buffer):\n","        return buffer.transpose((3, 0, 1, 2))\n","\n","    def load_frames(self, file_dir):\n","        frames = sorted([os.path.join(file_dir, img) for img in os.listdir(file_dir)])\n","        frame_count = len(frames)\n","        buffer = np.empty((frame_count, self.resize_height, self.resize_width, 3), np.dtype('float32'))\n","        for i, frame_name in enumerate(frames):\n","            frame = np.array(cv2.imread(frame_name)).astype(np.float64)\n","            buffer[i] = frame\n","\n","        return buffer\n","\n","    def crop(self, buffer, clip_len, crop_size):\n","        # randomly select time index for temporal jittering\n","        time_index = np.random.randint(buffer.shape[0] - clip_len)\n","\n","        # Randomly select start indices in order to crop the video\n","        height_index = np.random.randint(buffer.shape[1] - crop_size)\n","        width_index = np.random.randint(buffer.shape[2] - crop_size)\n","\n","        # Crop and jitter the video using indexing. The spatial crop is performed on\n","        # the entire array, so each frame is cropped in the same location. The temporal\n","        # jitter takes place via the selection of consecutive frames\n","        buffer = buffer[time_index:time_index + clip_len,\n","                 height_index:height_index + crop_size,\n","                 width_index:width_index + crop_size, :]\n","\n","        return buffer\n","    def getlabel2index(self):\n","        return self.label2index\n","\n","\n","\n","\n","\n","if __name__ == \"__main__\":\n","    from torch.utils.data import DataLoader\n","    train_data = VideoDataset(dataset='ucf101', split='train', clip_len=16, preprocess=False)\n","    train_loader = DataLoader(train_data, batch_size=1, shuffle=True, num_workers=0)\n","\n","    for i, sample in enumerate(train_loader):\n","        inputs = sample[0]\n","        labels = sample[1]\n","        print(inputs.size())\n","        print(labels)\n","\n","        if i == 1:\n","            break"],"metadata":{"id":"ZjYMaLX1yBV2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q2\n","Read the dataset.py file and answer how this code prepare the video data and the label."],"metadata":{"id":"YuvF5nsYyHEI"}},{"cell_type":"markdown","source":["*Your Answer*"],"metadata":{"id":"LHh9hGKnymTs"}},{"cell_type":"markdown","source":["### Q3\n","Read the paper and reproduce the C3D network in C3D_model.py in network folder. (You need to implement the init and the forward function of C3D class.)"],"metadata":{"id":"9VHphu5BzG_Q"}},{"cell_type":"markdown","source":["### C3D.py\n"],"metadata":{"id":"lIA9B53AzZz4"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from mypath import Path\n","\n","class C3D(nn.Module):\n","    \"\"\"\n","    The C3D network.\n","    \"\"\"\n","\n","    def __init__(self, num_classes, pretrained=False):\n","        super(C3D, self).__init__()\n","\t# implement your code here\n","\n","        \n","\n","        self.__init_weight()\n","\n","        if pretrained:\n","            self.__load_pretrained_weights()\n","\n","    def forward(self, x):\n","\n","        #implement your code here\n","\n","        return logits\n","\n","    def __load_pretrained_weights(self):\n","        \"\"\"Initialiaze network.\"\"\"\n","        corresp_name = {\n","                        # Conv1\n","                        \"features.0.weight\": \"conv1.weight\",\n","                        \"features.0.bias\": \"conv1.bias\",\n","                        # Conv2\n","                        \"features.3.weight\": \"conv2.weight\",\n","                        \"features.3.bias\": \"conv2.bias\",\n","                        # Conv3a\n","                        \"features.6.weight\": \"conv3a.weight\",\n","                        \"features.6.bias\": \"conv3a.bias\",\n","                        # Conv3b\n","                        \"features.8.weight\": \"conv3b.weight\",\n","                        \"features.8.bias\": \"conv3b.bias\",\n","                        # Conv4a\n","                        \"features.11.weight\": \"conv4a.weight\",\n","                        \"features.11.bias\": \"conv4a.bias\",\n","                        # Conv4b\n","                        \"features.13.weight\": \"conv4b.weight\",\n","                        \"features.13.bias\": \"conv4b.bias\",\n","                        # Conv5a\n","                        \"features.16.weight\": \"conv5a.weight\",\n","                        \"features.16.bias\": \"conv5a.bias\",\n","                         # Conv5b\n","                        \"features.18.weight\": \"conv5b.weight\",\n","                        \"features.18.bias\": \"conv5b.bias\",\n","                        # fc6\n","                        \"classifier.0.weight\": \"fc6.weight\",\n","                        \"classifier.0.bias\": \"fc6.bias\",\n","                        # fc7\n","                        \"classifier.3.weight\": \"fc7.weight\",\n","                        \"classifier.3.bias\": \"fc7.bias\",\n","                        }\n","\n","        p_dict = torch.load(Path.model_dir())\n","        s_dict = self.state_dict()\n","        for name in p_dict:\n","            if name not in corresp_name:\n","                continue\n","            s_dict[corresp_name[name]] = p_dict[name]\n","        self.load_state_dict(s_dict)\n","\n","    def __init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv3d):\n","                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                # m.weight.data.normal_(0, math.sqrt(2. / n))\n","                torch.nn.init.kaiming_normal_(m.weight)\n","            elif isinstance(m, nn.BatchNorm3d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","def get_1x_lr_params(model):\n","    \"\"\"\n","    This generator returns all the parameters for conv and two fc layers of the net.\n","    \"\"\"\n","    b = [model.conv1, model.conv2, model.conv3a, model.conv3b, model.conv4a, model.conv4b,\n","         model.conv5a, model.conv5b, model.fc6, model.fc7]\n","    for i in range(len(b)):\n","        for k in b[i].parameters():\n","            if k.requires_grad:\n","                yield k\n","\n","def get_10x_lr_params(model):\n","    \"\"\"\n","    This generator returns all the parameters for the last fc layer of the net.\n","    \"\"\"\n","    b = [model.fc8]\n","    for j in range(len(b)):\n","        for k in b[j].parameters():\n","            if k.requires_grad:\n","                yield k\n","\n","if __name__ == \"__main__\":\n","    inputs = torch.rand(1, 3, 16, 112, 112)\n","    net = C3D(num_classes=101, pretrained=True)\n","\n","    outputs = net.forward(inputs)\n","    print(outputs.size())\n"],"metadata":{"id":"IdVM-umAzd_3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q4\n","Read the train.py. Feel free to train the model if you have GPU resources (GPU memory: at least 2GB). If you do not have GPU resources, I have prepared a pretrained model (C3D-ucf101_epoch-19.pth.tar). If you need it please put it in the “run/run_0/model/” folder. Then explain the experiment details of train.py in your report. I spend about 1 hour on a RTX 3090 GPU with batch size=64. Please find the model files (c3d-init.pth and C3D-ucf101_epoch-19.pth.tar) on google drive and put c3d-init.pth in the “models” folder.(https://drive.google.com/drive/folders/0AIf6uQcTDj-LUk9PVA)"],"metadata":{"id":"CzEZ-betzfca"}},{"cell_type":"markdown","source":["## Q5\n","Q5\n","\n","Test your model and provide one example of action recognition like demo1.gif in assets folder. I provide a test code, but it cannot generate the demo1.gif, add a function in your test code so that it can generate example of action recognition."],"metadata":{"id":"FH-AVcIxzxwO"}},{"cell_type":"markdown","source":["Please submit your jpynb and report to mycourse. Please do not submit your model file and dataset."],"metadata":{"id":"ltJfcxE5z1NL"}},{"cell_type":"code","source":[""],"metadata":{"id":"l8uARKXFz-BB"},"execution_count":null,"outputs":[]}]}